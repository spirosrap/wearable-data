{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Pulse Rate Algorithm\n",
    "\n",
    "### Contents\n",
    "Fill out this notebook as part of your final project submission.\n",
    "\n",
    "**You will have to complete both the Code and Project Write-up sections.**\n",
    "- The [Code](#Code) is where you will write a **pulse rate algorithm** and already includes the starter code.\n",
    "   - Imports - These are the imports needed for Part 1 of the final project. \n",
    "     - [glob](https://docs.python.org/3/library/glob.html)\n",
    "     - [numpy](https://numpy.org/)\n",
    "     - [scipy](https://www.scipy.org/)\n",
    "- The [Project Write-up](#Project-Write-up) to describe why you wrote the algorithm for the specific case.\n",
    "\n",
    "\n",
    "### Dataset\n",
    "You will be using the **Troika**[1] dataset to build your algorithm. Find the dataset under `datasets/troika/training_data`. The `README` in that folder will tell you how to interpret the data. The starter code contains a function to help load these files.\n",
    "\n",
    "1. Zhilin Zhang, Zhouyue Pi, Benyuan Liu, ‘‘TROIKA: A General Framework for Heart Rate Monitoring Using Wrist-Type Photoplethysmographic Signals During Intensive Physical Exercise,’’IEEE Trans. on Biomedical Engineering, vol. 62, no. 2, pp. 522-531, February 2015. Link\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "a = [1]*200\n",
    "b = [2]*200\n",
    "kf = KFold()\n",
    "splits = kf.split(a,b)\n",
    "for i, (train_idx, test_idx) in enumerate(splits):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor\n",
    "from sklearn.model_selection import KFold, LeaveOneGroupOut\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "def LoadTroikaDataset():\n",
    "    \"\"\"\n",
    "    Retrieve the .mat filenames for the troika dataset.\n",
    "\n",
    "    Review the README in ./datasets/troika/ to understand the organization of the .mat files.\n",
    "\n",
    "    Returns:\n",
    "        data_fls: Names of the .mat files that contain signal data\n",
    "        ref_fls: Names of the .mat files that contain reference data\n",
    "        <data_fls> and <ref_fls> are ordered correspondingly, so that ref_fls[5] is the \n",
    "            reference data for data_fls[5], etc...\n",
    "    \"\"\"\n",
    "    data_dir = \"./datasets/troika/training_data\"\n",
    "    data_fls = sorted(glob.glob(data_dir + \"/DATA_*.mat\"))\n",
    "    ref_fls = sorted(glob.glob(data_dir + \"/REF_*.mat\"))\n",
    "    return data_fls, ref_fls\n",
    "\n",
    "def LoadTroikaDataFile(data_fl):\n",
    "    \"\"\"\n",
    "    Loads and extracts signals from a troika data file.\n",
    "\n",
    "    Usage:\n",
    "        data_fls, ref_fls = LoadTroikaDataset()\n",
    "        ppg, accx, accy, accz = LoadTroikaDataFile(data_fls[0])\n",
    "\n",
    "    Args:\n",
    "        data_fl: (str) filepath to a troika .mat file.\n",
    "\n",
    "    Returns:\n",
    "        numpy arrays for ppg, accx, accy, accz signals.\n",
    "    \"\"\"\n",
    "    data = sp.io.loadmat(data_fl)['sig']\n",
    "    return data[2:]\n",
    "\n",
    "def load_data(data_fl, ref_fl):\n",
    "    # Sampling rate    \n",
    "    fs=125\n",
    "    # Window to calculate reference pulse rate\n",
    "    win_len = 6\n",
    "    # Difference between time windows\n",
    "    win_shift = 2\n",
    "    \n",
    "    # sig = scipy.io.loadmat(data_fl)[\"sig\"]\n",
    "    sig = LoadTroikaDataFile(data_fl)\n",
    "    ref = scipy.io.loadmat(ref_fl)[\"BPM0\"]\n",
    "    ref = np.array([x[0] for x in ref])\n",
    "    subject_name = os.path.basename(data_fl).split('.')[0]        \n",
    "    start_indxs, end_indxs = get_indxs(sig.shape[1], len(ref), fs, win_len,win_shift)\n",
    "    targets, features, sigs, subs = [], [], [], []\n",
    "    for i, s in enumerate(start_indxs):\n",
    "        start_i =  start_indxs[i]\n",
    "        end_i = end_indxs[i]\n",
    "\n",
    "        # Take the ppg as the mean of ppg signals from the two wrists.\n",
    "#         ppg = np.mean(sig[1:3, start_i:end_i], axis = 0)\n",
    "#         ppg1 = sig[1, start_i:end_i]\n",
    "        ppg = sig[0, start_i:end_i]            \n",
    "        accx = sig[1, start_i:end_i]\n",
    "        accy = sig[2, start_i:end_i]\n",
    "        accz = sig[3, start_i:end_i]\n",
    "\n",
    "        # Bandpass all signals\n",
    "        ppg = bandpass_filter(ppg)\n",
    "#         ppg1 = bandpass_filter(ppg1)\n",
    "#         ppg2 = bandpass_filter(ppg2)\n",
    "\n",
    "        accx = bandpass_filter(accx)\n",
    "        accy = bandpass_filter(accy)\n",
    "        accz = bandpass_filter(accz)\n",
    "\n",
    "        feature, ppg, accx, accy, accz = featurize(ppg, accx, accy, accz)\n",
    "\n",
    "        sigs.append([ppg, accx, accy, accz])\n",
    "        targets.append(ref[i])\n",
    "        features.append(feature)\n",
    "        subs.append(subject_name)\n",
    "        \n",
    "    return (np.array(targets), np.array(features), sigs, subs)\n",
    "\n",
    "def load_data2():\n",
    "    # Sampling rate    \n",
    "    fs=125\n",
    "    # Window to calculate reference pulse rate\n",
    "    win_len = 6\n",
    "    # Difference between time windows\n",
    "    win_shift = 2\n",
    "    \n",
    "    data_fls, ref_fls = LoadTroikaDataset()\n",
    "    pbar = tqdm(list(zip(data_fls, ref_fls)), desc=\"Data Preparation\")\n",
    "    targets, features, sigs, subs = [], [], [], []\n",
    "    for data_fl, ref_fl in pbar:\n",
    "        # sig = scipy.io.loadmat(data_fl)[\"sig\"]\n",
    "        sig = LoadTroikaDataFile(data_fl)\n",
    "        ref = scipy.io.loadmat(ref_fl)[\"BPM0\"]\n",
    "        ref = np.array([x[0] for x in ref])\n",
    "        subject_name = os.path.basename(data_fl).split('.')[0]        \n",
    "        start_indxs, end_indxs = get_indxs(sig.shape[1], len(ref), fs, win_len,win_shift)\n",
    "        for i, s in enumerate(start_indxs):\n",
    "            start_i =  start_indxs[i]\n",
    "            end_i = end_indxs[i]\n",
    "\n",
    "            # Take the ppg as the mean of ppg signals from the two wrists.\n",
    "    #         ppg = np.mean(sig[1:3, start_i:end_i], axis = 0)\n",
    "    #         ppg1 = sig[1, start_i:end_i]\n",
    "            ppg = sig[0, start_i:end_i]            \n",
    "            accx = sig[1, start_i:end_i]\n",
    "            accy = sig[2, start_i:end_i]\n",
    "            accz = sig[3, start_i:end_i]\n",
    "\n",
    "            # Bandpass all signals\n",
    "            ppg = bandpass_filter(ppg)\n",
    "    #         ppg1 = bandpass_filter(ppg1)\n",
    "    #         ppg2 = bandpass_filter(ppg2)\n",
    "\n",
    "            accx = bandpass_filter(accx)\n",
    "            accy = bandpass_filter(accy)\n",
    "            accz = bandpass_filter(accz)\n",
    "\n",
    "            feature, ppg, accx, accy, accz = featurize(ppg, accx, accy, accz)\n",
    "\n",
    "            sigs.append([ppg, accx, accy, accz])\n",
    "            targets.append(ref[i])\n",
    "            features.append(feature)\n",
    "            subs.append(subject_name)\n",
    "        \n",
    "    return (np.array(targets), np.array(features), sigs, subs)\n",
    "\n",
    "\n",
    "def AggregateErrorMetric(pr_errors, confidence_est):\n",
    "    \"\"\"\n",
    "    Computes an aggregate error metric based on confidence estimates.\n",
    "\n",
    "    Computes the MAE at 90% availability. \n",
    "\n",
    "    Args:\n",
    "        pr_errors: a numpy array of errors between pulse rate estimates and corresponding \n",
    "            reference heart rates.\n",
    "        confidence_est: a numpy array of confidence estimates for each pulse rate\n",
    "            error.\n",
    "\n",
    "    Returns:\n",
    "        the MAE at 90% availability\n",
    "    \"\"\"\n",
    "    # Higher confidence means a better estimate. The best 90% of the estimates\n",
    "    #    are above the 10th percentile confidence.\n",
    "    percentile90_confidence = np.percentile(confidence_est, 10)\n",
    "\n",
    "    # Find the errors of the best pulse rate estimates\n",
    "    best_estimates = pr_errors[confidence_est >= percentile90_confidence]\n",
    "\n",
    "    # Return the mean absolute error\n",
    "    return np.mean(np.abs(best_estimates))\n",
    "\n",
    "def featurize(ppg, accx, accy, accz):\n",
    "    \"\"\" Create features \"\"\"\n",
    "    ppg = bandpass_filter(ppg)\n",
    "    accx = bandpass_filter(accx)\n",
    "    accy = bandpass_filter(accy)\n",
    "    accz = bandpass_filter(accz)\n",
    "    \n",
    "    \n",
    "    # Fourier Transform and the frequency domain\n",
    "    fs = 125\n",
    "    n = len(ppg) * 4\n",
    "    freqs = np.fft.rfftfreq(n, 1/fs)\n",
    "    fft = np.abs(np.fft.rfft(ppg,n))\n",
    "    fft[freqs <= 40/60.0] = 0.0\n",
    "    fft[freqs >= 240/60.0] = 0.0\n",
    "    \n",
    "    # L2-Norms of acc\n",
    "    l2_acc = np.sqrt(accx**2 + accy**2 + accz**2)\n",
    "    \n",
    "    # FFT for acc\n",
    "    acc_fft = np.abs(np.fft.rfft(l2_acc, n))\n",
    "    acc_fft[freqs <= 40/60.0] = 0.0\n",
    "    acc_fft[freqs >= 240/60.0] = 0.0\n",
    "    \n",
    "    # max frequency for ppg as a feature\n",
    "    ppg_feature = freqs[np.argmax(fft)]\n",
    "    # max frequency for L2 norm of acc\n",
    "    acc_feature = freqs[np.argmax(acc_fft)]\n",
    "    \n",
    "    return (np.array([ppg_feature, acc_feature]), ppg, accx, accy, accz)\n",
    "\n",
    "def regressor_preparation(features, targets, subs):\n",
    "    \"\"\" The regression model\"\"\"\n",
    "    AdaBoostRegressor\n",
    "    regression = RandomForestRegressor(n_estimators=400,max_depth=16)\n",
    "    scores = []\n",
    "    lf = KFold(n_splits=5)\n",
    "#     lf = LeaveOneGroupOut()\n",
    "    splits = lf.split(features,targets,subs)\n",
    "    for i, (train_idx, test_idx) in enumerate(splits):\n",
    "        X_train, y_train = features[train_idx], targets[train_idx]\n",
    "        X_test, y_test = features[test_idx], targets[test_idx]\n",
    "        regression.fit(X_train, y_train)\n",
    "        y_pred = regression.predict(X_test)\n",
    "        score = error_score(y_test, y_pred)\n",
    "        scores.append(score)\n",
    "        \n",
    "    return (regression, scores)\n",
    "\n",
    "def Evaluate():\n",
    "    \"\"\"\n",
    "    Top-level function evaluation function.\n",
    "\n",
    "    Runs the pulse rate algorithm on the Troika dataset and returns an aggregate error metric.\n",
    "\n",
    "    Returns:\n",
    "        Pulse rate error on the Troika dataset. See AggregateErrorMetric.\n",
    "    \"\"\"\n",
    "    # Retrieve dataset files\n",
    "    data_fls, ref_fls = LoadTroikaDataset()\n",
    "    errs, confs = [], []\n",
    "    for data_fl, ref_fl in zip(data_fls, ref_fls):\n",
    "        # Run the pulse rate algorithm on each trial in the dataset\n",
    "        errors, confidence = RunPulseRateAlgorithm(data_fl, ref_fl)\n",
    "        errs.append(errors)\n",
    "        confs.append(confidence)\n",
    "        # Compute aggregate error metric\n",
    "    errs = np.hstack(errs)\n",
    "    confs = np.hstack(confs)\n",
    "    return AggregateErrorMetric(errs, confs)\n",
    "\n",
    "def bandpass_filter(signal):\n",
    "    \"\"\"Bandpass filter the signal between 40 and 240 BPM\"\"\"    \n",
    "    pass_band=(40/60.0, 240/60.0)\n",
    "    fs = 125\n",
    "    b, a = scipy.signal.butter(3, pass_band, btype='bandpass', fs=fs)\n",
    "    return scipy.signal.filtfilt(b, a, signal)\n",
    "\n",
    "def get_indxs(sig_len, ref_len, fs=125, win_len_s=10, win_shift_s=2):\n",
    "    \"\"\"\n",
    "    Find start and end index to iterate over a set of signals\n",
    "    \"\"\"\n",
    "    # Set the length of the biggest signal with regards to the reference signal\n",
    "    if ref_len < sig_len:\n",
    "        n = ref_len\n",
    "    else:\n",
    "        n = sig_len\n",
    "    \n",
    "    # Start Indexes \n",
    "    start_indxs = (np.cumsum(np.ones(n) * fs * win_shift_s) - fs * win_shift_s).astype(int)\n",
    "    \n",
    "    # End Indexes (same size as the start indexes array)\n",
    "    end_indxs = start_indxs + win_len_s * fs\n",
    "    return (start_indxs, end_indxs)\n",
    "\n",
    "def predict(reg,feature, ppg, accx, accy, accz):\n",
    "    \"\"\"Predict based on the regressor\"\"\"\n",
    "    est = reg.predict(np.reshape(feature, (1, -1)))[0]\n",
    "    \n",
    "def error_score(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate error score of a prediction\n",
    "    \"\"\"\n",
    "    return mean_squared_error(y_test, y_pred)\n",
    "\n",
    "\n",
    "def load_regressor():\n",
    "    fname = \"outfile.npy\"\n",
    "    reg, scores = [], []\n",
    "    if os.path.isfile(fname):\n",
    "        [reg,scores] = np.load(fname,allow_pickle=True)\n",
    "        print(\"1\")\n",
    "    else:\n",
    "        targets, features, sigs, subs = load_data2()\n",
    "        reg, scores = regressor_preparation(features, targets, subs)\n",
    "        np.save(\"outfile\", [reg,scores])\n",
    "    return reg, scores\n",
    "\n",
    "def RunPulseRateAlgorithm(data_fl, ref_fl):\n",
    "    # Sample Frequency\n",
    "    fs = 125\n",
    "    # Window to calculate reference pulse rate\n",
    "    win_len = 8\n",
    "    # Difference between time windows\n",
    "    win_shift = 2    \n",
    "       \n",
    "    reg, scores = load_regressor()\n",
    "    targets, features, sigs, subs = load_data(data_fl, ref_fl)\n",
    "\n",
    "    error, confidence = [], []\n",
    "    for i,feature in enumerate(features):\n",
    "        est = reg.predict(np.reshape(feature, (1, -1)))[0]\n",
    "        \n",
    "        # Calculate confidence\n",
    "        ppg, accx, accy, accz = sigs[i]\n",
    "        \n",
    "        ppg = bandpass_filter(ppg)        \n",
    "        accx = bandpass_filter(accx)\n",
    "        accy = bandpass_filter(accy)\n",
    "        accz = bandpass_filter(accz)        \n",
    "        \n",
    "        n = len(ppg) * 3\n",
    "        freqs = np.fft.rfftfreq(n, 1/fs)\n",
    "        fft = np.abs(np.fft.rfft(ppg,n))\n",
    "        fft[freqs <= 40/60.0] = 0.0\n",
    "        fft[freqs >= 240/60.0] = 0.0\n",
    "    \n",
    "        # Max magnitude frequency\n",
    "        est_fs = est / 55.0\n",
    "        fs_win = 30  / 60.0\n",
    "        fs_win_e = (freqs >= est_fs - fs_win) & (freqs <= est_fs +fs_win)\n",
    "        conf = np.sum(fft[fs_win_e])/np.sum(fft)\n",
    "        \n",
    "        error.append(np.abs((est-targets[i])))\n",
    "        confidence.append(conf)\n",
    "    return np.array(error), np.array(confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data Preparation: 100%|██████████| 12/12 [00:05<00:00,  2.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.596599594278816"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm outfile.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Write-up\n",
    "\n",
    "\n",
    "#### 1. **Code Description**\n",
    "\n",
    "*Include details so someone unfamiliar with your project will know how to run your code and use your algorithm.*\n",
    "\n",
    "I have built an algorithm to predict heart rate measured in beats per minute (BPM). To estimate the BPM there are two kinds of signals the PPG signal (photoplethysmographic) measured by the two wrists (two signals) and IMU signals (x,y,z dimensions) from the Inertial Measurement unit (for better accuracy). Also, there's the \"ECG\" signal which is not yet used in this part of the project. In my code I used only the second PPG signal which is more noisy.\n",
    "\n",
    "The code is available in the jupyter notebook and returns the mean absolute error and the confidence (of the final estimation) as a 2-tuple of numpy arrays.\n",
    "\n",
    "It can be easily changed to also return the heart rate estimation for use in projects.\n",
    "\n",
    "#### 2. **Data Description**\n",
    "\n",
    "*Describe the dataset that was used to train and test the algorithm. Include its short-comings and what data would be required to build a more complete dataset.*\n",
    "\n",
    "The Troika data set is used to build the algorithm. The training data is placed in the folder `nd320-c4-wearable-data-project-starter/part_1/datasets/troika/training_data` (https://arxiv.org/pdf/1409.5181.pdf)\n",
    "\n",
    "Regarding the signals:\n",
    "\n",
    "* ECG signal with one channel\n",
    "* PPG two signals from each wrist (Only one is used)\n",
    "* Three channels fro the accelerometer each one corresponding to (x,y and z).\n",
    "* The data is 125Hz sampled.\n",
    "\n",
    "#### 2. **Algorithhm Description**\n",
    "\n",
    "##### How the algorithm works\n",
    "\n",
    "The algorithm uses regression (`RandomForestRegressor`) to fit the heart rate training data.\n",
    "\n",
    "##### The specific aspects of the physiology that it takes advantage of\n",
    "\n",
    "We use the PPG signals from the two wrists. The capillaries in the wrist fill with blood when the heart's ventricles contract. When the blood returns to the heart there are fewer blood cells in the wrist. The PPG sensor emits a green light which can be absorbed by the red blood cells and the photodetector will see the various levels of blood flow in the reflected light. When the blood returns to the heart  fewer blood cells can absorb the green light and the photo detector can see an increase in the reflected light.\n",
    "\n",
    "##### A description of the algorithm outputs\n",
    "\n",
    "The algorithm returns the predicted heart rate (BPM) and the confidence rate of this prediction. More confidence rate means that we can trust the prediction more.\n",
    "\n",
    "##### Caveats on algorithm outputs\n",
    "\n",
    "The confidence rate is calculated based on the magnitude of a small area that contains the estimated spectral frequency relative to the sum magnitude of the entire spectrum.\n",
    "\n",
    "##### Common failure modes\n",
    "\n",
    "Sometimes the PPG picks a higher frequency that is not from the heart rate. This could be from the hand movements To deal with this problem, we take into consideration the accelerometer values.\n",
    "\n",
    "\n",
    "#### 3. **Algorithm Performance**\n",
    "\n",
    "*Detail how performance was computed (eg. using cross-validation or train-test split) and what metrics were optimized for. Include error metrics that would be relevant to users of your algorithm. Caveat your performance numbers by acknowledging how generalizable they may or may not be on different datasets.*\n",
    "\n",
    "We calculated the mean absolute error of the heart rate estimation and the ground truth reference signal from the ECG sensors.\n",
    "\n",
    "For cross validation I used the `LeaveOneGroupOut()` or `KFold` cross vilidation.\n",
    "\n",
    "The error rate was around 8 BPM on the test set.  Since the data is using only limited subjects the algorithm may not be able to generalize well.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 4. **Clinical Conclusion**\n",
    "\n",
    "1. For women, we see higher heart rate for 35 to 65\n",
    "2. For men, we see a steady rates for 35 to 75\n",
    "3. In comparison to men, women's heart rate has more variance\n",
    "4. What are some possible reasons for what we see in our data?\n",
    "  * The women sample is 4 times lower than of men (277 whereas men 1260). Also, we may have gotten the sample for a specific demographic (for example, office clerks).\n",
    "5. What else can we do or go and find to figure out what is really happening? How would that improve the results?\n",
    "  * Repeat the experiments with more data.\n",
    "6. Did we validate the trend that average resting heart rate increases up until middle age and then decreases into old age? How?\n",
    "  * No because the sample is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Next Steps\n",
    "You will now go to **Test Your Algorithm** (back in the Project Classroom) to apply a unit test to confirm that your algorithm met the success criteria. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. **Test**\n",
    "\n",
    "I was able to pass the test with a heart error rate around 10BPM:\n",
    "\n",
    "![pass](../pass.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
